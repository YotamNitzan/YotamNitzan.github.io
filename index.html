<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yotam Nitzan</title>

    <meta name="author" content="Yotam Nitzan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">

            <!-- Header and image -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <!-- Header -->
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Yotam Nitzan</name>
                        </p>
                        <p>I am a Ph.D. student in Computer Science at Tel-Aviv Univeristy, advised by <a
                                href="https://danielcohenor.com/"> Prof. Daniel Cohen-Or</a>.
                            My research interests are machine learning, computer vision and computer graphics.</p>
                        <p>I'm currently interning at Google Research working with <a
                                href="https://kfiraberman.github.io/"> Kfir Aberman</a>.</p>
                        <p>
                            Previously, I received M.Sc. (summa cum laude) in Computer Science from Tel-Aviv Univeristy
                            and B.Sc. (cum laude) in Applied Mathematics from Bar-Ilan University.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:yotamnitzan@gmail.com">Email</a> &nbsp/&nbsp
                            <a href="https://github.com/YotamNitzan">Github</a> &nbsp/&nbsp
                            <!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
                            <!--                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp-->
                            <a href="https://scholar.google.com/citations?hl=en&user=pTUX5wEAAAAJ">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://twitter.com/YotamNitzan">Twitter</a>
                        </p>
                    </td>

                    <!-- Image -->
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo" src="images/YotamNitzan.jpg"
                             class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Research header -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Research</heading>
                        <p>
                            <!--                I'm interested in computer vision, machine learning, optimization, and image processing.-->
                            <!--                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.-->
                            <!--                Representative papers are <span class="highlight">highlighted</span>.-->
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Publications -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!-- MyStyle -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video width="200" autoplay muted loop>
                            <source src="images/mystyle-teaser.mp4" type="video/mp4">
                        </video>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2203.17272">
                            <papertitle>MyStyle: A Personalized Generative Prior</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
                        <a href="https://research.google/people/QiuruiCharlesHe/">Qiurui He</a>,
                        <a href="https://sites.google.com/view/orly-liba">Orly Liba</a>,
                        <a href="michalyarom@google.com">Michal Yarom</a>,
                        <a href="https://yossi.gandelsman.com/">Yossi Gandelsman</a>,
                        <a href="https://research.google.com/pubs/InbarMosseri.html">Inbar Mosseri</a>,
                        <a href="https://research.google/people/106214/">Yael Pritch</a>,
                        <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>

                        <br>
                        <em>arXiv</em>, 2022
                        <br>
                        <a href="https://mystyle-personalized-prior.github.io/">project page</a> /
                        <a href="https://arxiv.org/abs/2203.17272">arXiv</a> /
                        <a href="https://youtu.be/QvOdQR3tlOc">video</a> /
                        <p></p>
                        <p>
                            We train the first personalized face generator, trained for a specific individual from ~100 images of them.
                            The model now holds a personalized prior, faithfully representing their unique appearance.
                            We then leverage the personalized prior to solve a number of ill-posed image enhancement and editing tasks.
                        </p>
                    </td>
                </tr>

                <!-- StyleAlign -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video width="200" autoplay muted loop>
                            <source src="images/style-align-teaser.mp4" type="video/mp4">
                        </video>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2110.11323">
                            <papertitle>StyleAlign: Analysis and Applications of Aligned StyleGAN Models</papertitle>
                        </a>
                        <br>
                        <a href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>
                        <br>
                        <em>ICLR</em>, 2022 (Oral)
                        <br>
                        <a href="https://arxiv.org/abs/2110.11323">arXiv</a> /
                        <a href="https://www.youtube.com/watch?v=gjjo11IncP4">video</a> /
                        <a href="https://github.com/betterze/StyleAlign">code</a>
                        <p></p>
                        <p>
                            Two models as considered aligned if they share the same architecture, and one of them (the
                            child) is obtained
                            from the other (the parent) via fine-tuning to another domain, a common practice in transfer
                            learning.
                            In this paper, we perform an in-depth study of the properties and applications of aligned
                            generative models.
                        </p>
                    </td>
                </tr>

                <!-- LARGE -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/LARGE.PNG" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2107.11186">
                            <papertitle>LARGE: Latent-Based Regression through GAN Semantics</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan*</strong>,
                        <a href="https://rinongal.github.io/">Rinon Gal*</a>,
                        <a>Ofri Brenner</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>CVPR</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2107.11186">arXiv</a> /
                        <a href="https://github.com/YotamNitzan/LARGE">code</a>
                        <p></p>
                        <p>
                            We propose a novel method for solving regression tasks using few-shot or weak supervision.
                            At the core
                            of our method is the observation that the distance of a latent code from a semantic
                            hyperplane is roughly
                            linearly correlated with the magnitude of the said semantic property in the image
                            corresponding to the latent code.
                        </p>
                    </td>
                </tr>

                <!-- e4e -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/e4e-teaser.jpg" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2102.02766">
                            <papertitle>Designing an Encoder for StyleGAN Image Manipulation</papertitle>
                        </a>
                        <br>
                        <a>Omer Tov</a>,
                        <a href="https://scholar.google.com/citations?user=uvaPP80AAAAJ&hl=en">Yuval Alaluf</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://scholar.google.com/citations?hl=en&user=-SlS0mgAAAAJ">Or Patashnik</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>SIGGRAPH</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.02766">arXiv</a> /
                        <a href="https://github.com/omertov/encoder4editing">code</a>
                        <p></p>
                        <p>
                            We identify the existence of distortion-editability and distortion-perception tradeoffs
                            within the
                            StyleGAN latent space on inverted images. Accordingly, we suggest two principles for
                            designing
                            encoders that are suitable for facilitating editing on real images by balancing these
                            tradeoffs.
                        </p>
                    </td>
                </tr>

                <!-- pSp -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/pSp-teaser.jpg" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2008.00951">
                            <papertitle>Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=9npMV2kAAAAJ&hl=en">Elad Richardson</a>,
                        <a href="https://scholar.google.com/citations?user=uvaPP80AAAAJ&hl=en">Yuval Alaluf</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=-SlS0mgAAAAJ">Or Patashnik</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://scholar.google.com/citations?user=jKnGFpAAAAAJ&hl=en">Yaniv Azar</a>,
                        <a href="https://www.researchgate.net/scientific-contributions/Stav-Shapiro-2160567618">Stav
                            Shapiro</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>CVPR</em>, 2021
                        <br>
                        <a href="https://eladrich.github.io/pixel2style2pixel/">project page</a> /
                        <a href="https://arxiv.org/abs/2008.00951">arXiv</a> /
                        <a href="https://github.com/eladrich/pixel2style2pixel">code</a>
                        <p></p>
                        <p>
                            A generic image-to-image framework, based on an encoder that directly maps
                            into the latent space of a pretrained generator, StyleGAN2.
                        </p>
                    </td>
                </tr>

                <!-- ID Disen -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/SIGA-2020-teaser.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://yotamnitzan.github.io/ID-disentanglement/">
                            <papertitle>Face Identity Disentanglement via Latent Space Mapping</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://www.cs.tau.ac.il/~amberman/">Amit Bermano</a>,
                        <a href="http://yangyan.li/">Yangyan Li</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>SIGGRAPH Asia</em>, 2020
                        <br>
                        <a href="https://yotamnitzan.github.io/ID-disentanglement/">project page</a> /
                        <a href="https://arxiv.org/abs/2005.07728">arXiv</a> /
                        <a href="https://github.com/YotamNitzan/ID-disentanglement">code</a>
                        <p></p>
                        <p>
                            We propose to disentangle identity from other facial
                            attributes by mapping directly into the latent space of a pretrained
                            generator, StyleGAN.
                        </p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Footer -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                Website template courtesy of <a href="http://jonbarron.info/">Jon Barron.</a>
                            </font>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
