<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yotam Nitzan</title>

    <meta name="author" content="Yotam Nitzan">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">

            <!-- Header and image -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <!-- Header -->
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p style="text-align:center">
                            <name>Yotam Nitzan</name>
                        </p>
                        <p>I am a Research Scientist at Adobe in San Francisco. </p>
                        <p>
                            Previously, I earned my Ph.D. in Computer Science from Tel Aviv University, where I was advised by <a
                                href="https://danielcohenor.com/">Prof. Daniel Cohen-Or</a>. I've interned at Google Research and Adobe Research (x2).
                            Before that, I received an M.Sc. in Computer Science from Tel Aviv University
                            and a B.Sc. in Applied Mathematics from Bar-Ilan University.
                        </p>
                        <p>
                            I'm focused on visual generative models, aiming to make them more interactive, intuitive, and controllable. 
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:ynitzan@adobe.com">Email</a> &nbsp/&nbsp
                            <a href="https://github.com/YotamNitzan">GitHub</a> &nbsp/&nbsp
                            <a href="https://scholar.google.com/citations?hl=en&user=pTUX5wEAAAAJ">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://twitter.com/YotamNitzan">Twitter</a>
                        </p>
                    </td>

                    <!-- Image -->
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <img style="width:100%;max-width:100%" alt="profile photo" src="images/me_circle.png"
                             class="hoverZoomLink">
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Research header -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <heading>Selected Publications</heading>
                        <p>
                            <!--                I'm interested in computer vision, machine learning, optimization, and image processing.-->
                            <!--                Much of my research is about inferring the physical world (shape, motion, color, light, etc) from images.-->
                            <!--                Representative papers are <span class="highlight">highlighted</span>.-->
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Publications -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <!--Self-E-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/selfE.png" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://www.arxiv.org/abs/2512.22374">
                            <papertitle>Self-Evaluation Unlocks Any-Step Text-to-Image Generation</papertitle>
                        </a>
                        <br>
                        
                        <a href="https://xinyu-andy.github.io/">Xin Yu</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=bGn0uacAAAAJ&view_op=list_works&sortby=pubdate">Xiaojuan Qi</a>,
                        <a href="https://zhengqili.github.io/">Zhengqi Li</a>,
                        <a href="https://kai-46.github.io/website/">Kai Zhang</a>,
                        <a href="https://richzhang.github.io/">Richard Zhang</a>,
                        <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
                        <a href="https://scholar.google.com/citations?user=B_FTboQAAAAJ">Eli Shechtman</a>,
                        <a href="https://stevewongv.github.io/">Tianyu Wang</a>,
                        <strong>Yotam Nitzan</strong>
                        <br>
                        <em>arXiv</em>, 2025
                        <br>
                        <a href="https://xinyu-andy.github.io/SelfE-project/">project page</a> /
                        <a href="https://www.arxiv.org/abs/2512.22374">arXiv</a>
                        <br>
                        <p></p>
                    </td>
                </tr>
                <!--NP Edit-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/npedit_gif.gif" width="200">

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2510.14978">
                            <papertitle>Learning an Image Editing Model without Image Editing Pairs</papertitle>
                        </a>
                        <br>

                        <a href="https://nupurkmr9.github.io/" target="_blank">Nupur Kumari</a>,
                        <a href="https://peterwang512.github.io" target="_blank">Sheng-Yu Wang</a>,
                        <a href="https://www.nxzhao.com" target="_blank">Nanxuan Zhao</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://yuheng-li.github.io" target="_blank">Yuheng Li</a>,
                        <a href="https://krsingh.cs.ucdavis.edu" target="_blank">Krishna Kumar Singh</a>,
                        <a href="http://richzhang.github.io" target="_blank">Richard Zhang</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/" target="_blank">Eli Shechtman</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/" target="_blank">Jun-Yan Zhu</a>,
                        <a href="https://www.xunhuang.me" target="_blank">Xun Huang</a>
                        <br>
                        <em>arXiv</em>, 2025
                        <br>
                        <a href="https://nupurkmr9.github.io/npedit/">project page</a> /
                        <a href="https://arxiv.org/abs/2510.14978">arXiv</a>
                        <br>
                        <p></p>
                    </td>
                </tr>
                <!--VLM-GNP-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/creative_neg_prompt.jpeg" width="200">

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2510.10715">
                            <papertitle>VLM-Guided Adaptive Negative Prompting for Creative Generation</papertitle>
                        </a>
                        <br>

                        <a href="https://scholar.google.com/citations?user=_MHRaNIAAAAJ&hl=iw&oi=ao">Shelly Golan</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://betterze.github.io/website/">Zongze Wu</a>,
                        <a href="https://orpatashnik.github.io">Or Patashnik</a>
                        <br>
                        <em>arXiv</em>, 2025
                        <br>
                        <a href="https://shelley-golan.github.io/VLM-Guided-Creative-Generation/">project page</a> /
                        <a href="https://arxiv.org/abs/2510.10715">arXiv</a>
                        <br>
                        <p></p>
                    </td>
                </tr>
                <!--SSM_WM-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/ssm_wm.png" width="200">

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://www.arxiv.org/abs/2505.20171">
                            <papertitle>Long-Context State-Space Video World Models</papertitle>
                        </a>
                        <br>

                        <a href="https://ryanpo.com">Ryan Po</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://richzhang.github.io/">Richard Zhang</a>,
                        <a href="https://scholar.google.com/citations?user=GkCmZ18AAAAJ&hl=en">Berlin Chen</a>,
                        <a href="https://tridao.me/">Tri Dao</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>,
                        <a href="https://www.xunhuang.me/">Xun Huang</a>
                        <br>
                        <em>ICCV</em>, 2025
                        <br>
                        <a href="https://ryanpo.com/ssm_wm/">project page</a> /
                        <a href="https://www.arxiv.org/abs/2505.20171">arXiv</a>
                        <br>
                        <p></p>
                    </td>
                </tr>
                <!--LazyD-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/diffcr.png" width="200">

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2412.16822">
                            <papertitle>Layer-and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers</papertitle>
                        </a>
                        <br>

                        <a href="http://haoranyou.com">Haoran You</a>,
                        <a href="https://www.connellybarnes.com/work/">Connelly Barnes</a>,
                        <a href="https://yzhouas.github.io/">Yuqian Zhou</a>,
                        <a href="https://research.adobe.com/person/yan-kang/">Yan Kang</a>,
                        <a href="https://zhenbangdu.github.io/">Zhenbang Du</a>,
                        <a href="https://scholar.google.com/citations?user=KR1byjoAAAAJ">Wei Zhou</a>,
                        <a href="https://owenzlz.github.io/">Lingzhi Zhang</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://scholar.google.com/citations?user=KLVw2_UAAAAJ&hl=en">Xiaoyang Liu</a>,
                        <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://scholar.google.com/citations?user=aFrtZOIAAAAJ&hl=en">Sohrab Amirghodsi</a>,
                        <a href="https://eiclab.scs.gatech.edu/pages/team.html">Yingyan (Celine) Lin</a>
                        <br>
                        <em>CVPR</em>, 2025
                        <br>
                        <a href="https://www.haoranyou.com/diffcr/">project page</a> /
                        <a href="https://arxiv.org/abs/2412.16822">arXiv</a>
                        <br>
                        <p></p>
                    </td>
                </tr>

                <!--LazyD-->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/lazyd-teaser.png" width="200">

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2404.12382">
                            <papertitle>Lazy Diffusion Transformer for Interactive Image Editing</papertitle>
                        </a>
                        <br>

                        <strong>Yotam Nitzan</strong>,
                        <a href="https://research.adobe.com/person/zongze-wu/">Zongze Wu</a>,
                        <a href="http://richzhang.github.io/">Richard Zhang</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>,
                        <a href="https://taesung.me/">Taesung Park</a>,
                        <a href="http://www.mgharbi.com/">Michaël Gharbi</a>
                        <br>
                        <em>ECCV</em>, 2024
                        <br>
                        <a href="https://lazydiffusion.github.io/">project page</a> /
                        <a href="https://arxiv.org/abs/2404.12382">arXiv</a>
                        <br>
                        <p></p>
                        <!-- <p>
                            LazyDiffusion is an efficient image editing architecture that updates only user-specified regions.
                            Using a context encoder and a diffusion-based transformer decoder, it balances global context awareness with localized generation,
                            achieving state-of-the-art quality and up to 10× speed improvements.
                        </p> -->
                    </td>
                </tr>

                <!-- Expansion -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video width="200" autoplay muted loop>
                            <source src="images/expansion-teaser.mp4" type="video/mp4">
                        </video>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2301.05225">
                            <papertitle>Domain Expansion of Image Generators</papertitle>
                        </a>
                        <br>

                        <strong>Yotam Nitzan</strong>,
                        <a href="http://www.mgharbi.com/">Michaël Gharbi</a>,
                        <a href="http://richzhang.github.io/">Richard Zhang</a>,
                        <a href="https://taesung.me/">Taesung Park</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                        <br>
                        <em>CVPR</em>, 2023
                        <br>
                        <a href="https://yotamnitzan.github.io/domain-expansion/">project page</a> /
                        <a href="https://arxiv.org/abs/2301.05225">arXiv</a> /
                        <a href="https://github.com/adobe-research/domain-expansion">code</a>
                        <br>
                        <p></p>
                        <!-- <p>
                            We present a method to expand the generated domain of a pretrained generator while
                            respecting its existing structure and knowledge. To this end, we identify dormant regions
                            of the model's latent space and repurpose only them to model new concepts.
                        </p> -->
                    </td>
                </tr>


                <!-- STAR -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/star.jpg" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2202.14020">
                            <papertitle>State-of-the-Art in the Architecture, Methods and Applications of StyleGAN</papertitle>
                        </a>
                        <br>
                        <a href="https://www.cs.tau.ac.il/~amberman/">Amit H Bermano</a>,
                        <a href="https://rinongal.github.io/">Rinon Gal</a>,
                        <a href="https://yuval-alaluf.github.io/">Yuval Alaluf</a>,
                        <a href="https://il.linkedin.com/in/ron-mokady-665b5091">Ron Mokady</a>,
                        <strong>Yotam Nitzan</strong>,
                        <br>
                        <a href="https://scholar.google.com/citations?user=lbo_R54AAAAJ&hl=en">Omer Tov</a>,
                        <a href="https://orpatashnik.github.io/">Or Patashnik</a>,
                        <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>
                        <br>
                        <em>Eurographics</em>, 2022 (STARs)
                        <br>
                        <a href="https://arxiv.org/abs/2202.14020">arXiv</a>
                        <p></p>
                        <!-- <p>
                            We survey the family of StyleGAN models and how they have been employed
                            for downstream applications since their inception.
                        </p> -->
                    </td>
                </tr>

                <!-- MyStyle -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video width="200" autoplay muted loop>
                            <source src="images/mystyle-teaser.mp4" type="video/mp4">
                        </video>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2203.17272">
                            <papertitle>MyStyle: A Personalized Generative Prior</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
                        <a href="https://research.google/people/QiuruiCharlesHe/">Qiurui He</a>,
                        <a href="https://sites.google.com/view/orly-liba">Orly Liba</a>,
                        <a href="michalyarom@google.com">Michal Yarom</a>,
                        <a href="https://yossi.gandelsman.com/">Yossi Gandelsman</a>,
                        <a href="https://research.google.com/pubs/InbarMosseri.html">Inbar Mosseri</a>,
                        <a href="https://research.google/people/106214/">Yael Pritch</a>,
                        <a href="https://www.cs.tau.ac.il/~dcor/">Daniel Cohen-Or</a>

                        <br>
                        <em>SIGGRAPH Asia</em>, 2022 (Journal Track)
                        <br>
                        <a href="https://mystyle-personalized-prior.github.io/">project page</a> /
                        <a href="https://arxiv.org/abs/2203.17272">arXiv</a> /
                        <a href="https://youtu.be/axWo_9Gt47o">video</a> /
                        <a href="https://github.com/google/mystyle">code</a>
                        <p></p>
                        <!-- <p>
                            We present the first personalized face generator, trained for a specific individual from
                            ~100 of their images.
                            The model now holds a personalized prior, faithfully representing their unique appearance.
                            We then leverage the personalized prior to solve a number of ill-posed image enhancement and
                            editing tasks.
                        </p> -->
                    </td>
                </tr>

                <!-- StyleAlign -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <video width="200" autoplay muted loop>
                            <source src="images/style-align-teaser.mp4" type="video/mp4">
                        </video>

                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2110.11323">
                            <papertitle>StyleAlign: Analysis and Applications of Aligned StyleGAN Models</papertitle>
                        </a>
                        <br>
                        <a href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>
                        <br>
                        <em>ICLR</em>, 2022 (Oral)
                        <br>
                        <a href="https://arxiv.org/abs/2110.11323">arXiv</a> /
                        <a href="https://www.youtube.com/watch?v=gjjo11IncP4">video</a> /
                        <a href="https://github.com/betterze/StyleAlign">code</a>
                        <p></p>
                        <!-- <p>
                            Two models as considered aligned if they share the same architecture, and one of them (the
                            child) is obtained
                            from the other (the parent) via fine-tuning to another domain, a common practice in transfer
                            learning.
                            In this paper, we perform an in-depth study of the properties and applications of aligned
                            generative models.
                        </p> -->
                    </td>
                </tr>

                <!-- LARGE -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/LARGE.PNG" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2107.11186">
                            <papertitle>LARGE: Latent-Based Regression through GAN Semantics</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan*</strong>,
                        <a href="https://rinongal.github.io/">Rinon Gal*</a>,
                        <a>Ofir Brenner</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>CVPR</em>, 2022
                        <br>
                        <a href="https://arxiv.org/abs/2107.11186">arXiv</a> /
                        <a href="https://github.com/YotamNitzan/LARGE">code</a>
                        <p></p>
                        <!-- <p>
                            We propose a novel method for solving regression tasks using few-shot or weak supervision.
                            At the core
                            of our method is the observation that the distance of a latent code from a semantic
                            hyperplane is roughly
                            linearly correlated with the magnitude of the said semantic property in the image
                            corresponding to the latent code.
                        </p> -->
                    </td>
                </tr>

                <!-- e4e -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/e4e-teaser.jpg" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2102.02766">
                            <papertitle>Designing an Encoder for StyleGAN Image Manipulation</papertitle>
                        </a>
                        <br>
                        <a>Omer Tov</a>,
                        <a href="https://scholar.google.com/citations?user=uvaPP80AAAAJ&hl=en">Yuval Alaluf</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://scholar.google.com/citations?hl=en&user=-SlS0mgAAAAJ">Or Patashnik</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>SIGGRAPH</em>, 2021
                        <br>
                        <a href="https://arxiv.org/abs/2102.02766">arXiv</a> /
                        <a href="https://github.com/omertov/encoder4editing">code</a>
                        <p></p>
                        <!-- <p>
                            We identify the existence of distortion-editability and distortion-perception tradeoffs
                            within the
                            StyleGAN latent space on inverted images. Accordingly, we suggest two principles for
                            designing
                            encoders that are suitable for facilitating editing on real images by balancing these
                            tradeoffs.
                        </p> -->
                    </td>
                </tr>

                <!-- pSp -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/pSp-teaser.jpg" width="200">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://arxiv.org/abs/2008.00951">
                            <papertitle>Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation
                            </papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=9npMV2kAAAAJ&hl=en">Elad Richardson</a>,
                        <a href="https://scholar.google.com/citations?user=uvaPP80AAAAJ&hl=en">Yuval Alaluf</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=-SlS0mgAAAAJ">Or Patashnik</a>,
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://scholar.google.com/citations?user=jKnGFpAAAAAJ&hl=en">Yaniv Azar</a>,
                        <a href="https://www.researchgate.net/scientific-contributions/Stav-Shapiro-2160567618">Stav
                            Shapiro</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>CVPR</em>, 2021
                        <br>
                        <a href="https://eladrich.github.io/pixel2style2pixel/">project page</a> /
                        <a href="https://arxiv.org/abs/2008.00951">arXiv</a> /
                        <a href="https://github.com/eladrich/pixel2style2pixel">code</a>
                        <p></p>
                        <!-- <p>
                            A generic image-to-image framework, based on an encoder that directly maps
                            into the latent space of a pretrained generator, StyleGAN2.
                        </p> -->
                    </td>
                </tr>

                <!-- ID Disen -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img src="images/SIGA-2020-teaser.png" width="160">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://yotamnitzan.github.io/ID-disentanglement/">
                            <papertitle>Face Identity Disentanglement via Latent Space Mapping</papertitle>
                        </a>
                        <br>
                        <strong>Yotam Nitzan</strong>,
                        <a href="https://www.cs.tau.ac.il/~amberman/">Amit Bermano</a>,
                        <a href="http://yangyan.li/">Yangyan Li</a>,
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>
                        <br>
                        <em>SIGGRAPH Asia</em>, 2020
                        <br>
                        <a href="https://yotamnitzan.github.io/ID-disentanglement/">project page</a> /
                        <a href="https://arxiv.org/abs/2005.07728">arXiv</a> /
                        <a href="https://github.com/YotamNitzan/ID-disentanglement">code</a>
                        <p></p>
                        <!-- <p>
                            We propose to disentangle identity from other facial
                            attributes by mapping directly into the latent space of a pretrained
                            generator, StyleGAN.
                        </p> -->
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Footer -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                Website template courtesy of <a href="http://jonbarron.info/">Jon Barron.</a>
                            </font>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>

</html>
